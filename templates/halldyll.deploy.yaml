# Halldyll Deployment Configuration
# Documentation: https://github.com/halldyll/halldyll_deploy_pods

project:
  name: "my-project"
  environment: "dev"
  # region: "EU"  # Optional: EU, US, etc.
  cloud_type: SECURE  # SECURE or COMMUNITY
  compute_type: GPU

state:
  backend: local  # local or s3
  # For S3 backend:
  # bucket: "my-state-bucket"
  # prefix: "halldyll/my-project"
  # region: "us-east-1"

# Optional guardrails
# guardrails:
#   max_hourly_cost: 10.0  # Maximum hourly cost in USD
#   max_gpus: 4            # Maximum total GPUs
#   ttl_hours: 24          # Auto-stop after N hours
#   allow_gpu_fallback: false

pods:
  - name: "inference"
    gpu:
      type: "NVIDIA A40"
      count: 1
      # min_vram_gb: 40
      # fallback:
      #   - "NVIDIA L40S"
      #   - "NVIDIA RTX A6000"

    ports:
      - "22/tcp"
      - "8000/http"

    volumes:
      - name: "hf-cache"
        mount: "/root/.cache/huggingface"
        persistent: true
        # size_gb: 50

    runtime:
      image: "vllm/vllm-openai:latest"
      env:
        HF_TOKEN: "${HF_TOKEN}"
        # Add your environment variables here

    # Auto-download models and start inference engines
    models:
      - id: "llama-3-8b"
        provider: huggingface
        repo: "meta-llama/Meta-Llama-3-8B-Instruct"
        load:
          engine: vllm          # vllm, tgi, ollama, or transformers
          # quant: awq          # Optional: awq, gptq, fp8
          max_seq_len: 8192
          # options:            # Optional: engine-specific options
          #   tensor-parallel-size: 1

    health_check:
      endpoint: "/health"
      port: 8000
      interval_secs: 30
      timeout_secs: 5
      failure_threshold: 3

# Example: Multiple pods with different models
# - name: "vision-api"
#   gpu:
#     type: "NVIDIA A40"
#     count: 1
#   runtime:
#     image: "ghcr.io/huggingface/text-generation-inference:latest"
#   ports:
#     - "8000/http"
#   models:
#     - id: "llava-v1.6"
#       provider: huggingface
#       repo: "llava-hf/llava-v1.6-mistral-7b-hf"
#       load:
#         engine: tgi
